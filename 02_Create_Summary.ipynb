{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "from general_functions import *\n",
    "import glob, os\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-steal",
   "metadata": {},
   "source": [
    "# Create Article Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(article,tokenizer,model,max_length_s,n_beams):\n",
    "    ARTICLE_TO_SUMMARIZE = article\n",
    "    inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')#.to('cuda')\n",
    "\n",
    "    # Generate Summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=n_beams, max_length=max_length_s, early_stopping=False)\n",
    "    summary_text = ([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])    \n",
    "    \n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-sample",
   "metadata": {},
   "source": [
    "# Model Init and Run Over DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_summarization(model_name,file_name_in,max_length_s,n_beams,file_name_out):\n",
    "    \n",
    "    print(file_name_out)\n",
    "    \n",
    "    with open(file_name_in, 'rb') as handle:\n",
    "        DATA_LIST = pickle.load(handle)\n",
    "    \n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    #model.to('cuda')\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    example_counter = 0\n",
    "    final_list_all = []\n",
    "\n",
    "    for entry in DATA_LIST:\n",
    "        print(example_counter)    \n",
    "        article_mod = entry[2]\n",
    "        article = clean_article_new(entry[6],src_type)\n",
    "        highlights = clean_high_new(entry[7],src_type)\n",
    "\n",
    "        final_list_all.append([entry,highlights,summarize_article(article,tokenizer,model,max_length_s,n_beams),summarize_article(article_mod,tokenizer,model,max_length_s,n_beams)])\n",
    "        example_counter = example_counter + 1\n",
    "\n",
    "    with open(file_name_out, 'wb') as handle:\n",
    "        pickle.dump(final_list_all, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-whole",
   "metadata": {},
   "source": [
    "# Get All Picke Files to Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_in_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(os.getcwd()):\n",
    "    for file in files:\n",
    "        if file.find(\"pickle\") != -1:\n",
    "            if file.find(\"beams\") == -1:\n",
    "                f_name = str(os.path.join(root, file))\n",
    "                print(file)\n",
    "                file_name_in_list.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-circulation",
   "metadata": {},
   "source": [
    "# Run Over All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_s_list = [400]\n",
    "n_beams_list = [1,4,10]\n",
    "model_name_list = [\"facebook/bart-large-cnn\",\"facebook/bart-large-xsum\",\"sshleifer/distilbart-cnn-12-6\",\"sshleifer/distilbart-xsum-12-6\"]\n",
    "\n",
    "for model_name in model_name_list:  \n",
    "    for file_name_in in file_name_in_list:\n",
    "        for n_beams in n_beams_list:\n",
    "            for max_length_s in max_length_s_list:\n",
    "                \n",
    "                mname = model_name.replace('/','-')    \n",
    "\n",
    "                file_name_out = file_name_in + '_mLen_' + str(max_length_s) + '_name_'+mname+'_beams_'+str(n_beams)+'.pickle'\n",
    "\n",
    "                if file_name_in.find(\"cnn\") != -1:\n",
    "                    src_type = 'cnn_dailymail'\n",
    "\n",
    "                if file_name_in.find(\"multi\") != -1:\n",
    "                    src_type = 'multi_news'\n",
    "\n",
    "                if file_name_in.find(\"newsroom\") != -1:\n",
    "                    src_type = 'newsroom'\n",
    "\n",
    "                run_summarization(model_name,file_name_in,max_length_s,n_beams,file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-arbitration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
