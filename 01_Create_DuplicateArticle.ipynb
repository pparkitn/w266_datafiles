{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import spacy\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import pickle\n",
    "from general_functions import *\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-immigration",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG FOR HYPERPARAMETERS\n",
    "top_pos_list = [10] #TPOS and TNEG\n",
    "dup_cnt_list = [5]  #DNCT\n",
    "src_type_list = [\"newsroom\",\"cnn_dailymail\",\"multi_news\"]\n",
    "take_list = [1] #How many samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-journalist",
   "metadata": {},
   "source": [
    "# Get Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "def rank_sentence(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-syndication",
   "metadata": {},
   "source": [
    "# Get Top Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sent_idx(top_cnt,val_array):\n",
    "    \n",
    "    sent_idx = []\n",
    "    top_values = []\n",
    "    \n",
    "    array_sort = np.sort(val_array)\n",
    "    sent_cnt = 1\n",
    "        \n",
    "    if top_cnt > len(val_array):\n",
    "        top_cnt = len(val_array)\n",
    "        \n",
    "    while sent_cnt <= top_cnt:\n",
    "        top_value = array_sort[-(sent_cnt)]\n",
    "        top_values.append(top_value)\n",
    "        sent_cnt = sent_cnt + 1\n",
    "            \n",
    "    for value in top_values:\n",
    "        if value > 0.1:\n",
    "            sent_idx.append(val_array.index(value))\n",
    "    \n",
    "    return sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences(scores_array):\n",
    "    pos_array = []\n",
    "    neg_array = []\n",
    "    \n",
    "    for entry in scores_array:\n",
    "        pos_array.append(entry[0][2])\n",
    "        neg_array.append(entry[0][0])\n",
    "\n",
    "    pos_sent_idx = get_top_sent_idx(top_pos,pos_array)\n",
    "    neg_sent_idx = get_top_sent_idx(top_neg,neg_array)\n",
    "    \n",
    "    return neg_sent_idx,pos_sent_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-border",
   "metadata": {},
   "source": [
    "# Create Duplicate Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_dup(pos_sent_idx,neg_sent_idx,sentence_idx,highlights):\n",
    "\n",
    "    final_text = []\n",
    "        \n",
    "    for row in sentence_idx:\n",
    "\n",
    "        # Do it for positive sentences\n",
    "        try:\n",
    "            pos_idx_found = pos_sent_idx.index(row[0])\n",
    "            for i in range(dup_cnt):\n",
    "                final_text.append(row[1])\n",
    "        except ValueError:\n",
    "            e_r = 1\n",
    "        \n",
    "        # Do it for negative sentences\n",
    "        try:\n",
    "            neg_idx_found = neg_sent_idx.index(row[0])\n",
    "            for i in range(dup_cnt):\n",
    "                final_text.append(row[1])\n",
    "        except ValueError:\n",
    "            e_r = 1\n",
    "                \n",
    "        final_text.append(row[1])       \n",
    "    \n",
    "    final_text = ' '.join(final_text)\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article,highlights,counter):\n",
    "    scores_sent = []\n",
    "    sentence_idx = []\n",
    "    \n",
    "    article_str = clean_article_new(article,src_type)\n",
    "    doc = nlp(article_str)\n",
    "    \n",
    "    sent_counter = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_score = [0,0,0]\n",
    "        if len(sent.text) > 20:\n",
    "            sent_score = rank_sentence(sent.text)\n",
    "        scores_sent.append([sent_score,sent_counter])\n",
    "        sentence_idx.append([sent_counter,sent.text])\n",
    "        sent_counter = sent_counter + 1\n",
    "\n",
    "    #Find top Scores for sentences\n",
    "    neg_sent_idx,pos_sent_idx = get_top_sentences(scores_sent)        \n",
    "    final_text = create_article_dup(pos_sent_idx,neg_sent_idx,sentence_idx,str(highlights))\n",
    "        \n",
    "    return scores_sent, final_text , pos_sent_idx,neg_sent_idx,sentence_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentence_sentiment_process():\n",
    "    df = tfds.as_dataframe(ds.take(take),ds_info)\n",
    "\n",
    "    example_counter = 0\n",
    "    final_list_all = []\n",
    "\n",
    "    if src_type == 'multi_news':\n",
    "        article_k = \"document\"\n",
    "        summary_k = \"summary\"\n",
    "\n",
    "    if src_type == 'cnn_dailymail':\n",
    "        article_k = \"article\"\n",
    "        summary_k = \"highlights\"\n",
    "\n",
    "    if src_type == 'scientific_papers':\n",
    "        article_k = \"article\"\n",
    "        summary_k = \"abstract\"    \n",
    "\n",
    "    if src_type == 'billsum':\n",
    "        article_k = \"text\"\n",
    "        summary_k = \"summary\"        \n",
    "\n",
    "    if src_type == 'newsroom':\n",
    "        article_k = \"text\"\n",
    "        summary_k = \"summary\"    \n",
    "\n",
    "    if src_type == 'gigaword':\n",
    "        article_k = \"article\"\n",
    "        summary_k = \"headline\"     \n",
    "\n",
    "    for index,row in df.iterrows(): \n",
    "        print(index)\n",
    "\n",
    "        try:    \n",
    "            article = str(row[article_k])\n",
    "            highlights = str(row[summary_k])\n",
    "\n",
    "            scores_sent, final_text , pos_sent_idx,neg_sent_idx,sentence_idx = process_article(article,highlights,example_counter)\n",
    "            final_list_all.append([example_counter,scores_sent, final_text , pos_sent_idx,neg_sent_idx,sentence_idx,article,highlights])\n",
    "            example_counter = example_counter + 1    \n",
    "        except:\n",
    "            print(\"error\")\n",
    "    \n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(final_list_all, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-error",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN LOOP\n",
    "for top_pos in top_pos_list:\n",
    "    top_neg = top_pos\n",
    "    for dup_cnt in dup_cnt_list:\n",
    "        for src_type in src_type_list:\n",
    "            for take in take_list:\n",
    "                file_name = src_type + '_Pos_' +str(top_pos) + '_Neg_'+str(top_neg) + '_dCnt_' + str(dup_cnt) + '_take_' + str(take) +'.pickle'\n",
    "                print(file_name)\n",
    "\n",
    "                ds,ds_info = tfds.load(src_type, split='test', with_info=True) \n",
    "                assert isinstance(ds, tf.data.Dataset)\n",
    "\n",
    "                run_sentence_sentiment_process()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
